https://www.offidocs.com/loleaflet/dist/loleaflet.html?service=owncloudservice06&file_path=file:///var/www/html/weboffice/mydata/7175776569/NewDocuments/7274878166.docx&username=7175776569&ext=yes


https://www.offidocs.com/loleaflet/dist/loleaflet.html?service=owncloudservice06&file_path=file:///var/www/html/weboffice/mydata/7175776569/NewDocuments/7183907084.docx&username=7175776569&ext=yes

DATA VISUALIZATION
https://www.offidocs.com/loleaflet/dist/loleaflet.html?service=owncloudservice06&file_path=file:///var/www/html/weboffice/mydata/7175776569/NewDocuments/8277787165.docx&username=7175776569&ext=yes


https://www.offidocs.com/loleaflet/dist/loleaflet.html?service=owncloudservice06&file_path=file:///var/www/html/weboffice/mydata/7175776569/NewDocuments/7075868380.docx&username=7175776569&ext=yes
.
https://www.offidocs.com/loleaflet/dist/loleaflet.html?service=owncloudservice06&file_path=file:///var/www/html/weboffice/mydata/7175776569/NewDocuments/7075868380.docx&username=7175776569&ext=yes

https://journals.openedition.org/acrh/7376?file=1




Chapter 1
CONTEXTE GÉNÉRALE DU PROJET
L’objectif principal de ce chapitre est de présenter l’organisme d’accueil, son histoire et son fonctionnement. Le projet sera ensuite décrit, ainsi que ses objectifs, avant que le processus de réalisation ne soit expliqué.
1.1 Présentation de l’organisme d’accueil 1.1.1 Présentation générale du groupe SGMA
La Société Générale Maroc, filiale du groupe bancaire international Société Générale, occupe une place de choix dans le paysage financier marocain. Fortement ancrée dans l’économie nationale, cette institution bancaire joue un rôle primordial en offrant une gamme complète de services et de solutions financières aux particuliers, aux entreprises et aux institutions. Grâce à sa vision innovante et à son engagement envers l’excellence, la Société Générale Maroc s’efforce de répondre aux besoins évolutifs de ses clients et de contribuer au développement économique du Maroc.
Dans ce contexte dynamique, mon stage au sein de cette banque m’a offert l’opportunité unique de plonger au cœur des enjeux bancaires actuels et d’appréhender de près les défis et les opportunités qui caractérisent le secteur financier marocain.
1.1.2 Organigramme SGMA
Ci-dessous l’organigramme de Société Générale Maroc decrivant la structure de l’entreprise:
1

Chapter 1. CONTEXTE GÉNÉRALE DU PROJET
  Figure 1.1: Organigramme-SGMA
J’ai effectué mon projet au sein de la filière centrale COO. Spécialement dans la di- rection DATA BI.
1.1.3 La filière centrale COO
La filière centrale COO supervise trois périmètres : Technologie, Opérations et Ressources Générales, comme le montre l’organigramme 1.1. Ces derniers de leur tour sont consti- tués d’autres entités.
2

Chapter 1. CONTEXTE GÉNÉRALE DU PROJET
  Figure 1.2: Organigramme COO
Ces entités contribuent tous à réaliser les missions suivantes :
• Conduite de la transformation de la banque : Conduite, organisation et exécution de la transformation de la banque (Digitale, IT, Back Oﬀice et Immobilière).
• Centricité Client : Développement de solutions innovantes à destination du service client
• Animation des filières COO : Animation des filières COO au niveau des BU/SU et des délégations régionales.
• Eﬀicacité opérationnelle et nouvelles opportunités : Exploitation et anticipation des évolutions technologiques pour offrir de nouvelles opportunités business et d’être un acteur clef dans l’eﬀicacité opérationnelle, et ce, en fournissant des services, solutions ou organisations optimisés, à forte valeur ajoutée, résilients et au meilleur coût.
• RSE : Contribution active aux objectifs RSE de la banque et à la réduction de l’empreinte Carbonne.
1.2
1.2.1 1.2.2
Présentation du cadre de l’étude
Présentation du projet Contexte du projet
De nos jours, l’utilisation des techniques d’apprentissage automatique est devenue de plus en plus courante dans divers aspects de notre vie. La quantité considérable de données
3

Chapter 1. CONTEXTE GÉNÉRALE DU PROJET
 générées chaque jour permet de rendre les tâches et processus commerciaux traditionnels plus eﬀicaces et rationalisés. Dans ce contexte, le Groupe SGMA a décidé d’investir dans les sciences des données pour stimuler l’activation du canal digital de la banque, en encourageant davantage de clients à utiliser l’application mobile de la banque pour effectuer leurs opérations bancaires à distance.
L’activation du canal digital de la banque joue un rôle essentiel dans la génération de revenus pour les institutions financières, car elle permet aux clients de bénéficier de ser- vices bancaires pratiques et rapides via leur smartphone ou tablette. Cependant, bien que le Groupe SGMA figure parmi les leaders en termes du nombre de transactions par le canal digital selon les dernières données du marché, il existe encore un potentiel d’amélioration significatif.
1.2.3 Problématique
Actuellement, seulement 21% des clients utilisent régulièrement l’application mobile de la banque, mais il reste une part importante de clients qui n’ont pas encore adopté ce canal. Cela nous confronte à un défi important : comment encourager plus de clients à se tourner vers l’application mobile pour leurs opérations bancaires ?
Pour relever ce défi, le Groupe SGMA mise sur les techniques d’apprentissage au- tomatique et les sciences des données. En utilisant des modèles prédictifs et des analyses avancées, nous pouvons identifier les clients les plus susceptibles d’adopter l’application mobile pour leurs besoins bancaires. Cette approche nous permettra de mettre en œuvre des stratégies ciblées et des interventions personnalisées pour inciter ces clients spécifiques à utiliser le canal digital.
Le projet intitulé ”Modèle prédictif pour l’activation du canal digital de la banque” vise à développer un modèle robuste qui identifie avec précision les clients les plus enclins à adopter l’application mobile pour leurs transactions bancaires. En analysant les données historiques des clients et en appliquant des techniques avancées de prétraitement des données, nous serons en mesure de déterminer les caractéristiques pertinentes qui prédisent le mieux l’adoption du canal digital.
1.2.4 Objectifs
L’activation du canal digital au sein de la Société Générale Maroc (SGMA) présente une série d’avantages stratégiques qui contribuent à renforcer la position de la banque sur le marché financier marocain. En exploitant pleinement les opportunités offertes par la digitalisation, la SGMA peut :
• Améliorer l’Expérience Client : La digitalisation permet d’offrir aux clients une expérience bancaire plus fluide et personnalisée, grâce à la disponibilité en ligne des services, facilitant ainsi les transactions et les opérations quotidiennes.
• Élargir la Portée Géographique : Le canal digital transcende les limites géo- graphiques, permettant à la SGMA de toucher un public plus vaste, y compris les clients situés dans des régions éloignées.
4

Chapter 1. CONTEXTE GÉNÉRALE DU PROJET
 • Accroître l’Eﬀicacité Opérationnelle : Les services numériques automatisés réduisent les tâches manuelles et accélèrent les processus internes, améliorant ainsi l’eﬀicacité opérationnelle de la banque.
• Réduire les Coûts : La migration vers le canal digital diminue les coûts liés aux opérations en agence physique, tout en offrant une plus grande accessibilité aux services bancaires.
• Collecter des Données Précieuses : La digitalisation génère des données clients importantes, permettant à la SGMA d’obtenir des insights précieux pour améliorer ses offres, personnaliser les services et anticiper les besoins.
• Stimuler l’Innovation : La SGMA peut développer de nouveaux produits et services innovants, tels que des applications mobiles conviviales, des solutions de paiement avancées et des options de gestion de compte personnalisées.
• Renforcer la Compétitivité : L’adoption proactive de la digitalisation positionne la SGMA comme une institution financière moderne et compétitive, répondant aux attentes des clients actuels et futurs.
En somme, l’activation du canal digital représente une démarche stratégique incon- tournable pour la SGMA, favorisant une expérience client améliorée, des opérations plus eﬀicaces et une position renforcée sur le marché financier marocain en constante évolution.
1.2.5 Méthodologie de conduite du projet
Afin de bien mener ce projet, il était primordial de bien définir la méthodologie à adopter. Nous l’avons organisé en sept phases distinctes, chacune essentielle pour l’aboutissement réussi du projet.
1.2.5.1 Compréhension du besoin métier
Cette phase initiale du projet consistera à saisir pleinement les exigences et les attentes du métier. Des réunions avec la filiale marketing de la banque SGMA ainsi qu’avec des représentants de la direction de la Société Générale Maroc (Soge) seront organisées. L’objectif principal de ces réunions est d’obtenir une vision claire des problèmes actuels, des objectifs du projet, des données disponibles, et des mesures de succès attendues. Cette étape permettera de bien cerner le contexte global dans lequel le modèle prédictif sera utilisé et de définir les critères de performance du modèle.
1.2.5.2 Analyse des clients inactifs
Au cours du projet, une analyse approfondie des clients inactifs digitalement sera effectuée. Cette analyse permettera de comprendre le comportement des clients qui n’utilisent pas le canal digital pour leurs transactions bancaires. Les facteurs qui contribuent à l’inactivité digitale seront identifiés, et des insights pertinents ont été dégagés. Cette analyse fera
5

Chapter 1. CONTEXTE GÉNÉRALE DU PROJET
 également l’objet d’un livrable important sollicité par le pôle marketing pour orienter leurs stratégies d’activation des canaux digitaux.
1.2.5.3 Stratégie de développement
Nous débuterons par une analyse approfondie du taux de digitalisation au sein de SGMA, afin de cerner précisément le contexte et les enjeux. Cette analyse nous permettra de définir clairement l’objectif de notre modèle prédictif. En outre, nous établirons des objectifs de performance concrets que le modèle devra atteindre, notamment en termes de métriques d’évaluation. Ces objectifs serviront de référence pour évaluer l’eﬀicacité du modèle et guideront le processus de développement.
1.2.5.4 Préparation des données
Le chapitre de préparation des données constitue une étape fondamentale de notre pro- jet. Nous commencerons par présenter les données brutes sous forme tabulaire, en mettant en évidence les variables initiales ainsi que les variables développées spécifiquement pour notre modèle. Ensuite, nous expliquerons en détail la méthodologie de sélection des vari- ables, visant à réduire la dimensionnalité et gérer la multicolinéarité. Enfin, nous décrirons la construction de notre base de données d’entraînement, en appliquant des techniques de traitement des données pour garantir un ensemble de données équilibré et cohérent, prêt à être utilisé pour la modélisation.
1.2.5.5 Développement et Benchmarking
Une fois les méthodes de machine learning et de traitement de données choisies, cette phase consistera à développer le modèle prédictif en utilisant les données fournies par la banque SGMA et la Soge. Des jeux de données appropriés seront préparés et nettoyés en vue d’être utilisés pour l’entraînement et l’évaluation du modèle. Différents algorithmes de machine learning seront testés et ajustés afin de déterminer celui qui offre les meilleures performances en termes de prédiction. Un processus de benchmarking sera mis en place pour comparer les résultats obtenus par le modèle développé avec ceux de méthodes existantes ou de modèles prédictifs similaires utilisés dans d’autres institutions financières.
1.2.5.6 Evaluation et validation
Dans cette phase, le modèle prédictif sera testé sur une nouvelle donnée indépendante pour évaluer sa performance en termes de prédictions. Les résultats obtenus seront comparés avec les résultats réels pour mesurer la précision et la fiabilité du modèle. De plus, un modèle aléatoire sera également testé sur les mêmes données pour obtenir une référence de performance aléatoire. Cette comparaison permettra de déterminer l’eﬀicacité du mod- èle développé par rapport à une prédiction aléatoire et de valider sa pertinence pour l’activation du canal digital dans la banque SGMA.
6

Chapter 1. CONTEXTE GÉNÉRALE DU PROJET 1.2.5.7 Déploiement et automatisation
Une fois le modèle prédictif finalisé et évalué de manière satisfaisante, il sera déployé dans un environnement opérationnel au sein de la banque SGMA. Cette phase inclura égale- ment l’automatisation du processus pour que le modèle puisse être régulièrement mis à jour avec de nouvelles données et fournir des prédictions en temps réel. Des mécanismes de suivi et de reporting seront mis en place pour évaluer en continu les performances du modèle déployé, et des améliorations pourront être apportées au besoin.
La méthodologie décrite ci-dessus permettra de conduire le projet de manière structurée, en garantissant une compréhension approfondie des besoins métier, en tirant parti des meilleures pratiques de machine learning et de traitement de données, en développant un modèle performant, et en assurant un déploiement eﬀicace et une automatisation du processus pour une utilisation optimale par la banque SGMA. Cette approche itérative permettra également d’ajuster le modèle au fil du temps pour s’adapter aux évolutions du contexte bancaire et des besoins métier.
1.3 Conduite du projet 1.3.1 Méthodologie Agile
Le département DATA BI de la Société Générale Maroc opère dans un environnement Agile, favorisant la flexibilité, la collaboration et l’adaptation continue. L’approche Agile permet au département de réagir eﬀicacement aux changements, de répondre aux besoins des parties prenantes et d’assurer un développement itératif de projets. Les valeurs fon- damentales de l’Agile, telles que la communication constante avec les parties prenantes et la focalisation sur la livraison de valeur, guident les activités du département pour une conduite de projet eﬀicace et réactive.
1.3.2 Le framewor Scrumban
Au sein de l’environnement Agile du département DATA BI, le framework Scrumbam a été adopté pour structurer les processus de développement et de gestion de projets. Scrumbam combine les principes de Scrum et de Kanban, créant ainsi un équilibre entre la planification prévisionnelle et la gestion des flux de travail. Ce framework permet aux équipes du département de gérer eﬀicacement les itérations et les tâches en flux continu, en tirant parti des avantages des deux méthodologies. L’utilisation du framework Scrumbam favorise une approche itérative et adaptative, essentielle pour répondre aux besoins changeants du projet.
 7

Chapter 1. CONTEXTE GÉNÉRALE DU PROJET 1.3.3 Jira
Pour soutenir les pratiques Agile et le framework Scrumbam, l’équipe du département DATA BI utilise Jira comme principal outil de gestion de projet. Jira facilite la création, la planification, le suivi et la gestion des tâches, des user stories et des sprints. Grâce à son interface conviviale, Jira permet aux membres de l’équipe de collaborer en temps réel, de suivre les avancements et de prendre des décisions éclairées pour garantir le succès du projet.
Figure 1.3: Jira Application interface
1.3.4 Cycle de vie du projet (CRISP-DM)
Au sein du département DATA BI, l’approche méthodologique CRISP-DM (Cross-Industry Standard Process for Data Mining) est largement adoptée pour la conduite de projets an- alytiques et de data mining. Cette méthodologie structurée guide les équipes à travers les différentes phases du projet, de la compréhension des besoins métier à la mise en œuvre des solutions. Le processus CRISP-DM se décompose en six étapes clés : la compréhen- sion du domaine, la collecte des données, la préparation des données, la modélisation, l’évaluation et le déploiement. En suivant cette méthodologie éprouvée, l’équipe DATA BI bénéficie d’un cadre cohérent pour la gestion des projets, permettant une approche systématique tout en s’adaptant aux spécificités de chaque projet. L’utilisation de la méthode CRISP-DM assure une démarche rigoureuse dans l’analyse et l’exploitation des données, renforçant ainsi la qualité des résultats obtenus et l’alignement avec les objectifs métier.



Chapter 2
ANALYSE DE L’EXISTANT ET REVUE LITTERAIRE
10

Chapter 2. ANALYSE DE L’EXISTANT ET REVUE LITTERAIRE
 Ce chapitre vise à présenter la revue de littérature et l’étude de l’existant. Cela nous permettra de présenter l’environnement où le projet sera mené et définir toutes les notions théoriques utilisés dans ce dernier.
2.1 Analyse de l’existant :
Afin de fournir une vue d’ensemble de l’existant au sein du département DATA BI de la Société Générale Maroc (SGMA), nous explorerons la segmentation des clients selon dif- férents critères, les modèles préalablement développés, la chaîne de production analytique CI/CD et les canaux par lesquels les résultats des modèles sont diffusés, accompagnés d’exemples concrets.
2.1.1 Segmentation des Clients :
La segmentation des clients au sein de la Société Générale Maroc (SGMA) revêt une importance capitale pour mieux comprendre les besoins, les comportements et les carac- téristiques de chaque groupe. Cette segmentation permet d’adopter des stratégies adap- tées à chaque segment, renforçant ainsi la personnalisation des services offerts. Dans cet axe, les clients sont segmentés en fonction du marché auquel ils appartiennent, ainsi que de leur niveau. Voici les principaux segments :
2.1.1.1 Segmentation par Marché :
• Marché Clipri (Clients Particuliers) : Ce segment englobe les clients parti- culiers de la banque, qui constituent la base de la clientèle. Ils représentent une diversité de besoins allant des opérations courantes aux solutions d’épargne et de crédit.
• PME-PMI (Petites et Moyennes Entreprises) : Ce segment regroupe les clients professionnels, les institutions à but non lucratif et les TPE (Très Petites En- treprises). Ces clients ont des besoins spécifiques liés à leurs activités commerciales et professionnelles.
• PME-PMI (Petites et Moyennes Entreprises) : Les PME et PMI forment un segment distinct en raison de leur taille et de leurs besoins spécifiques en termes de financement, de gestion de trésorerie et de services bancaires dédiés.
• DGE (Grandes Entreprises) : Les grandes entreprises font partie de ce segment. Elles ont des exigences complexes en matière de services financiers, notamment pour la gestion de trésorerie, la gestion des risques et les opérations à grande échelle.
• DI (Institutionnels) : Les clients institutionnels, tels que les organismes gou- vernementaux, les institutions publiques et les organismes semi-publics, sont inclus dans ce segment. Leurs besoins sont souvent liés à des opérations de grande enver- gure.
11

Chapter 2. ANALYSE DE L’EXISTANT ET REVUE LITTERAIRE 2.1.1.2 Segmentation des Clients Particuliers :
Les clients particuliers sont ségmentés en 5 segments selon le leurs salaire et leurs PNB généré : Mass Market , Grand Public , Bonne Gamme , Haute Gammeet Patrimoniaux.
2.1.2 Modèles Préexistants :
Nous présenterons ici quelques-uns des modèles analytiques préalablement développés par le département DATA BI.
• Modèle Churn : L’objectif du modèle prédictif de la pré-attrition est d’anticiper la tombée en inactivité d’un client. Cela permet ainsi au chargé de clientèle de prioriser ses prises de contact pour renouer avec le client et chercher ainsi à revaloriser et renforcer la relation.
• Modèle Upgrade Carte : Le modèle permet d’identifier les clients équipés par des cartes basiques et qui sont appétant pour une montée en gamme carte
• Modèle LMV Equipement : Le modèle permet d’identifier les clients appétant pour souscrire à des produits d’assurance épargne: VITAL PROJET, VITAL ED- UCATION, VITAL RETRAITE et VITAL RETRAITE COMPLEMENTAIRE
• Modèle Dépot Clients : Le modèle dépôt client permet d’identifier des clients dont SGMA est banque secondaire afin de domicilier leur revenus et capter leur flux.
• Modèle Crédit Consomation : L’objectif est d’identifier les clients appétent aux crédit à la consommation.
2.1.3 Chaîne de Production Analytique CI/CD :
La chaîne de production analytique continue (CI/CD) est une composante clé du proces- sus de développement au sein du département. Cette chaîne permet de gérer eﬀicacement le déploiement des modèles en production. Elle intègre des pratiques de développement agiles, d’automatisation des tests, de déploiement et de suivi continu, assurant la qualité et la fiabilité des modèles déployés.
 12

Chapter 2. ANALYSE DE L’EXISTANT ET REVUE LITTERAIRE
  Figure 2.1: Chaine de production analytique CI/CD
• GitLab : GitLab est une plateforme de gestion du cycle de vie des applications, largement utilisée pour la gestion de projets de développement logiciel et la mise en œuvre de processus d’intégration continue et de déploiement continu (CI/CD). Son rôle essentiel au sein de la chaîne CI/CD est de permettre une gestion centralisée du code source, de la collaboration entre les développeurs, des tests automatisés, de la construction des applications et de leur déploiement. GitLab offre un en- semble d’outils et de fonctionnalités pour faciliter le développement, les tests et le déploiement fluides des applications, garantissant ainsi une eﬀicacité accrue et une qualité constante tout au long du processus de développement.
• Jenkins : Jenkins est un serveur d’intégration continue open-source largement utilisé pour automatiser et orchestrer les processus de développement, de tests et de déploiement des logiciels. Dans la chaîne CI/CD, Jenkins joue un rôle clé en automatisant les étapes du développement, telles que la compilation du code source, les tests unitaires, les tests de validation et le déploiement des applications. Il permet également de planifier et d’exécuter ces étapes de manière régulière, en s’intégrant avec diverses technologies et outils de développement. Jenkins facilite la détection rapide des erreurs et des problèmes, améliorant ainsi la qualité du logiciel tout en accélérant le processus de livraison et de déploiement.
• Nexus : Nexus est une plateforme de gestion de référentiels open-source conçue pour stocker, gérer et distribuer des artefacts de développement tels que des fichiers binaires, des bibliothèques, des packages et des dépendances. Dans le contexte de la chaîne CI/CD, Nexus joue un rôle crucial en tant que référentiel centralisé où les développeurs stockent et partagent leurs artefacts de manière sécurisée et organisée. Il permet de garantir la cohérence des versions et de réduire les erreurs liées à la gestion des dépendances et des packages. Nexus offre également des fonctionnal- ités de gestion des politiques de sécurité, de suivi des licences et de distribution automatisée des artefacts aux différentes étapes du processus de développement et de déploiement.
13

Chapter 2. ANALYSE DE L’EXISTANT ET REVUE LITTERAIRE 2.1.4 Environnement de Développement
L’équipe DATA-BI travaille dans un environnement de développement moderne et per- formant qui comprend en plus de la chaîne CI/CD, JupyterLab Server et le langage de programmation Python. Ces outils sont essentiels pour la mise en œuvre, le développe- ment et l’expérimentation des modèles de prédiction dans notre projet.
2.1.4.1 JupyterLab Server
JupyterLab Server est une interface de développement interactive basée sur un navigateur qui permet aux analystes et aux scientifiques des données de créer et de collaborer sur des documents contenant du code, des visualisations et du texte explicatif. Cet environnement offre une souplesse exceptionnelle pour exécuter et tester des scripts Python, ainsi que pour explorer et analyser les données en utilisant des notebooks interactifs.
Figure 2.2: JupyterLab
Grâce à JupyterLab Server, notre équipe peut facilement accéder aux ressources et aux données nécessaires pour la conception et l’entraînement de nos modèles. Il nous permet également de créer des présentations et des rapports riches en contenu en combinant du code, des résultats et des explications, ce qui favorise la communication eﬀicace des résultats et des découvertes.
2.1.4.2 Python
Python est le langage de programmation central utilisé par l’équipe DATA-BI pour la mise en œuvre des modèles de prédiction. Sa syntaxe conviviale, sa richesse en biblio- thèques spécialisées pour le traitement des données, la manipulation, la modélisation et la visualisation, en font un choix optimal pour nos besoins.
Nous utiliserons des bibliothèques Python telles que NumPy, pandas, scikit-learn, XGBoost, LightGBM et d’autres pour la préparation et l’analyse des données, la création et l’entraînement des modèles, ainsi que pour l’évaluation des performances.
  14

Chapter 2. ANALYSE DE L’EXISTANT ET REVUE LITTERAIRE
  Figure 2.3: Python
En somme, l’utilisation de JupyterLab Server et de Python comme environnement de développement nous permet de travailler de manière agile, eﬀicace et collaborative tout en produisant des modèles de prédiction performants et fiables pour notre projet de prédiction d’appétence au canal digital et de prévention de la désactivation digitale.
2.1.5 Canaux de diffusion :
La diffusion ciblée des résultats et des opportunités générées par les modèles prédictifs existants se fait à travers les canaux INTAJ et MOBIWAN, au sein de la Société Générale Maroc (SGMA).
2.1.5.1 INTAJ : Le Portail Commercial de la Banque :
INTAJ occupe une place centrale dans la diffusion des informations et des opportunités au sein des agences de la SGMA. En tant que portail commercial et CRM (Customer Relationship Management), INTAJ offre une vision complète à 360 degrés des clients. Il met à disposition des gestionnaires des listes de gestion, des prises de rendez-vous, et des indicateurs clés de qualité des données collectées. Plus important encore, INTAJ constitue une plateforme cruciale pour la diffusion des opportunités identifiées grâce aux modèles de la DATA BI. Les opportunités de ventes et de valorisation de produits, découlant des sorties des modèles prédictifs, sont aﬀichées sur INTAJ. Cela permet aux gestionnaires de clientèle de prendre des actions proactives pour cibler les clients les plus pertinents et d’optimiser les interactions commerciales. (figure )
15

Chapter 2. ANALYSE DE L’EXISTANT ET REVUE LITTERAIRE
  Figure 2.4: INTAJ
2.1.5.2 MOBIWAN : L’Application Mobile de la Banque
MOBIWAN joue un rôle significatif dans l’engagement client grâce à son interface in- tuitive et à ses fonctionnalités diverses. En tant qu’application mobile de la SGMA, MOBIWAN offre une expérience utilisateur conviviale, permettant aux clients d’accéder à leurs comptes, d’effectuer des opérations bancaires et de consulter des informations fi- nancières en temps réel. Plus précisément, MOBIWAN offre la possibilité de déployer des notifications push ciblées aux clients identifiés comme étant appétents à travers les modèles prédictifs. Cette fonctionnalité personnalisée permet de promouvoir des produits et des offres spécifiques, basées sur les besoins et les comportements anticipés des clients (figure).
Figure 2.5: Mobiwan
 16

Chapter 2. ANALYSE DE L’EXISTANT ET REVUE LITTERAIRE
  Figure 2.6: Notification
2.2 Revue litteraire
L’objectif de cette revue de littérature est de poser les bases solides sur lesquelles reposent nos choix méthodologiques et notre approche de développement. Nous mettrons en évi- dence les principales méthodes qui ont guidé notre travail, en nous concentrant sur celles qui sont intrinsèquement liées à notre projet, tout en intégrant les connaissances et les enseignements tirés de la littérature académique et professionnelle. Cette revue de littéra- ture fournira une orientation claire pour comprendre notre processus de développement de modèles et notre démarche d’amélioration continue.
2.2.1 L’apprentissage supervisé
En apprentissage supervisé, nous avons besoin de l’aide des données préalablement collec- tées afin de former nos modèles. Un modèle basé sur l’apprentissage supervisé nécessiterait à la fois des données et des résultats antérieurs en entrée. En s’entraînant avec ces don- nées, le modèle aide à prédire des résultats plus précis.
De plus, les données, que nous utilisons comme données d’entrée, sont également éti- quetées dans ce cas. En guise d’exemple, si un algorithme doit différencier les fruits, les données doivent être étiquetées ou classées pour différents fruits de la collection. Les données sont réparties en classes en apprentissage supervisé.
17

Chapter 2. ANALYSE DE L’EXISTANT ET REVUE LITTERAIRE
  Figure 2.7: Processus de l’apprentissage supervisé
La figure 2.7, présente un modèle d’apprentissage supervisé pour la classification des images des fruits (pomme), après l’entrainement sur les données pour l’association entre la photo et le nom du fruit, le modèle sera capable de prédire le nom (pomme). En effet, l’apprentissage supervisé est généralement effectué pour des problèmes de classification et de régression.
La partie suivante présentera l’ensemble des algorithmes de classification utilisés dans notre projet.
2.2.1.1 Logistic Regression
La régression logistique est un processus de modélisation de la probabilité d’un résultat discret compte tenu d’une variable d’entrée. La régression logistique la plus courante modélise un résultat binaire. Cette dernière estime la probabilité d’une classe en se basant sur une fonction d’hypothèse nomée Sigmoid dont l’expression mathématique est la suivante :
Et dont le graphe est le suivant:
 18

Chapter 2. ANALYSE DE L’EXISTANT ET REVUE LITTERAIRE
  2.2.1.2 Decision tree
Figure 2.8: Fonction Sigmoïd courbe
Les arbres de décision sont un type d’apprentissage automatique supervisé où les don- nées sont continuellement divisées en fonction d’un certain paramètre. L’arbre peut être expliquée par trois entités, à savoir les nœuds de décision, les branches et les feuilles. Cet algorithme peut être utilisé à la fois pour les problèmes de classification et de ré- gression, mais il est généralement préféré pour résoudre les problèmes de classification. Il s’agit d’un classificateur arborescent, où les nœuds de décision représentent les carac- téristiques d’un ensemble de données, les branches représentent les règles de décision et chaque nœud feuille représente le résultat.
Dans l’analyse de décision, un arbre de décision peut être utilisé pour représenter vi- suellement et explicitement les décisions et la prise de décision. Comme son nom l’indique, il utilise un modèle de décisions en forme d’arbre une arbre est divisé en plusieurs sours- arbres de décision est combine les resultats des chaque sub-tree c’est ce qu’on appelle le Bagging 2.10.
19

Chapter 2. ANALYSE DE L’EXISTANT ET REVUE LITTERAIRE
  Figure 2.9: Exemplaire d’arbre de décision
2.2.1.3 Random forest
La forêt aléatoire (Random Forest en anglais) est un algorithme d’apprentissage au- tomatique couramment utilisé qui combine le résultat de plusieurs arbres de décision pour obtenir un résultat unique. Sa facilité d’utilisation et sa flexibilité ont alimenté son adoption, car il gère à la fois les problèmes de classification et de régression.
 Figure 2.10: Examplaire Random forest tree (Bagging)
20

Chapter 2. ANALYSE DE L’EXISTANT ET REVUE LITTERAIRE 2.2.1.4 Extra Trees
L’algorithme Extra Trees (abrégé pour ”arbres extrêmement aléatoires”), similaire à l’algorithme Random Forest, crée plusieurs arbres de décision. Cependant, l’échantillonnage pour chaque arbre est aléatoire sans remplacement. Cela crée un ensemble de données unique pour chaque arbre avec des échantillons uniques. Un nombre spécifique de carac- téristiques, à partir de l’ensemble de caractéristiques, est également sélectionné aléatoire- ment pour chaque arbre. La caractéristique la plus importante et unique d’Extra Trees est la sélection aléatoire d’une valeur de division pour une caractéristique. Au lieu de calculer une valeur optimale locale en utilisant Gini ou l’entropie pour diviser les données, l’algorithme sélectionne aléatoirement une valeur de division. Cela diversifie les arbres et les rend non corrélés.
   Description Caractéristiques
Description Caractéristiques
2.2.1.5 XGBoost
Random Forest
Sous-ensembles d’échantillons par bootstrap,
les nœuds sont divisés en recherchant la meilleure division. Variance moyenne
Prend du temps pour trouver le meilleur nœud de division.
Extra Trees
Les arbres échantillonnent l’ensemble des données, divisions de nœuds aléatoires.
Basse variance
Plus rapide car les divisions de nœuds sont aléatoires.
Table 2.1: Random Forest vs. Extra Trees
     XGBoost est un algorithme d’apprentissage automatique haute performance qui améliore la précision prédictive en combinant des modèles faibles, souvent des arbres de décision, via le boosting. Il est largement utilisé pour diverses tâches en raison de son eﬀicacité, de sa gestion des données manquantes et de ses options personnalisables.
2.2.1.6 LightGBM
LightGBM est un framework de gradient boosting rapide et adapté qui dépend du calcul d’arbres de décision. Il est utilisé pour la classification, la caractérisation et d’autres tâches d’IA. Contrairement à d’autres calculs de boosting qui divisent la profondeur de l’arbre en deux parties, une sage et une perspicace, LightGBM divise la feuille de l’arbre le plus approprié. Ainsi, le calcul feuille par feuille dans LightGBM peut réduire davantage les erreurs et offrir une meilleure précision par rapport à d’autres calculs de boosting.
21

Chapter 2. ANALYSE DE L’EXISTANT ET REVUE LITTERAIRE
  Figure 2.11: Comparaison entre XGBoost et LightGBM
2.2.2 Données déséquilibré
La gestion de données déséquilibrées est cruciale pour prédire l’appétence des clients à un produits, car il est courant que le nombre de clients appétant soit nettement inférieur à celui des non-appétant. Pour résoudre ce problème, diverses techniques peuvent être utilisées pour équilibrer la distribution des classes.
• Poids des classes : Les poids des classes sont utilisés pour accorder plus d’importance à la classe minoritaire (appétant) pendant le processus d’entraînement. Cela est facilement applicable à divers algorithmes de classification, tels que la Régression Logistique, le Random Forest ou l’ExtraTrees. Par défaut, la valeur de class_weight est None, ce qui signifie que les deux classes ont un poids égal. Lorsque class_weights = ’balanced’, le modèle attribue automatiquement des poids de classe inversement proportionnels à leurs fréquences respectives.
Formule des poids de classe :
wj = n_samples n_classesn_samplesj
• Techniques de rééchantillonnage :
  Figure 2.12: Sur-échantillonnage par rapport à Sous-échantillonnage
22

Chapter 2. ANALYSE DE L’EXISTANT ET REVUE LITTERAIRE
 – Sur-échantillonnage : Augmentez le nombre d’échantillons de la classe mi- noritaire en répliquant les échantillons existants ou en générant des échantil- lons synthétiques (par exemple, en utilisant des techniques comme SMOTE, ADASYN - Adaptive Synthetic Sampling).
∗ SMOTE (Synthetic Minority Oversampling Technique) : SMOTE synthétise des éléments pour la classe minoritaire. L’algorithme fonctionne en sélectionnant des exemples proches dans l’espace des caractéristiques, en traçant une ligne entre les exemples dans l’espace des caractéristiques, puis en générant un nouvel échantillon à un point le long de cette ligne.
Figure 2.13: SMOTE
∗ ADASYN : ADASYN est une approche plus générique. Pour chaque observation de la classe minoritaire, il détermine d’abord l’impureté du voisinage en prenant le rapport entre les observations de la classe majori- taire dans le voisinage et k.
– Sous-échantillonnage : Réduisez le nombre d’échantillons de la classe ma- joritaire en supprimant aléatoirement des instances. Cependant, cela peut entraîner une perte d’informations.
∗ Liens de Tomek : sont une technique de sous-échantillonnage développée en 1976 par Ivan Tomek. Il s’agit d’une modification de la méthode des voisins les plus proches condensés (CNN). Il peut être utilisé pour trouver les échantillons de données souhaités de la classe majoritaire ayant la dis- tance euclidienne la plus faible avec les données de la classe minoritaire, puis les supprimer.
  Figure 2.14: Sous-échantillonnage par liens de Tomek
– Sur-échantillonnage combiné au sous-échantillonnage : SMOTETomek, comme exemple de combinaison de sous-échantillonnage et de sur-échantillonnage.
23

Chapter 2. ANALYSE DE L’EXISTANT ET REVUE LITTERAIRE
 Ajoute des touches délicates, crée de nouvelles instances pour la classe minori- taire et supprime les instances inutiles de la classe majoritaire, transformant la toile en un chef-d’œuvre plus harmonieux. Cette technique garantit que chaque couleur obtient sa part équitable, améliorant ainsi la précision de l’image finale peinte par les modèles prédictifs.
2.3 Optimisation des hyper-paramètres et cross vali- dation
L’optimisation ou le réglage des hyperparamètres est le problème du choix d’un ensem- ble d’hyperparamètres optimaux pour un algorithme d’apprentissage.
Un hyperparamètre est un paramètre dont la valeur est utilisée pour contrôler le pro- cessus d’apprentissage. En revanche, les valeurs des autres paramètres (généralement les poids des nœuds) sont apprises.
2.3.1 Random Search
La recherche aléatoire (RS) est une famille de méthodes d’optimisation numérique qui ne nécessite pas d’optimiser le gradient du problème, et RS peut donc être utilisée sur des fonctions non continues ou différentiables. Ces méthodes d’optimisation sont égale- ment connues sous le nom de méthodes de recherche directe, sans dérivée ou de black-box.
L’algorithme décrit un type de recherche aléatoire locale, où chaque itération dépend de la solution candidate de l’itération précédente. Il existe d’autres méthodes de recherche aléatoire qui échantillonnent à partir de l’intégralité de l’espace de recherche (par exemple, la recherche aléatoire pure ou la recherche aléatoire globale uniforme).
2.3.2 K-fold Cross validation
Dans la cross validation K Fold, les données sont divisées en k sous-ensembles. La méthode de retenue est ensuite répétée k fois, de sorte qu’à chaque fois, l’un des k sous- ensembles est utilisé comme ensemble de test/validation et les k-1 autres sous-ensembles sont réunis pour former un ensemble d’apprentissage. L’estimation de l’erreur est moyen- née sur l’ensemble des k essais pour obtenir l’eﬀicacité totale de notre modèle. Comme on peut le constater, chaque point de données se retrouve exactement une fois dans l’ensemble de validation et k-1 fois dans l’ensemble d’apprentissage. Cela réduit considérablement le biais, car nous utilisons la plupart des données pour l’ajustement, et réduit aussi consid- érablement la variance.
24

Chapter 2. ANALYSE DE L’EXISTANT ET REVUE LITTERAIRE
  Figure 2.15: Comparaison entre XGBoost et LightGBM
2.3.3 Data-centric & Model-centric
Data-centric et Model-centric sont tous les deux des approches pour améliorer les ré- sultats des modèles de machine learning.[14] Mettant l’accent sur la différence entre les deux :
• Data-centric : Cette approche centrée sur les données consiste à modifier ou à améliorer systématiquement notre jeu de données afin d’améliorer les performances du modèle. Cela signifie que, contrairement à l’approche centrée sur le modèle, cette fois-ci, le modèle est fixe et on ne fait qu’améliorer les données. L’amélioration de l’ensemble de données peut avoir différentes significations. Il peut s’agir de veiller à la cohérence des étiquettes, d’échantillonner finement les données d’apprentissage et de choisir judicieusement les lots, ce qui ne signifie pas toujours un effort pour augmenter la taille de l’ensemble de données.
• Model-centric : Cette approche centrée sur le modèle consiste à développer la recherche expérimentale pour améliorer les performances du modèle machine learn- ing. Cela implique de sélectionner la meilleure architecture de modèle et le meilleur processus de formation parmi un large éventail de possibilités. Dans cette approche, nous gardons les mêmes données et améliorons le code et les hyperparamètres du modèle.
25

Chapter 2. ANALYSE DE L’EXISTANT ET REVUE LITTERAIRE
  Figure 2.16: Data-centric vs Model-centric
2.3.4 Métriques d’évaluation pour une distribution de classes non équilibrée
Dans le contexte de la classification binaire avec une distribution de classes non équili- brée, il est crucial d’utiliser des métriques d’évaluation appropriées qui tiennent compte de la classe minoritaire. Les métriques traditionnelles telles que la précision, le rappel et le score F1 peuvent donner une image biaisée de la performance du modèle en présence de classes déséquilibrées.
2.3.4.1 Précision (Precision)
La précision mesure la proportion de prédictions positives correctes parmi toutes les prédictions positives faites par le modèle. Elle est définie comme le rapport entre les vrais positifs (VP) et la somme des vrais positifs et des faux positifs (FP):
Précision = V P VP+FP
2.3.4.2 Rappel (Recall)
Le rappel, également appelé sensibilité, mesure la proportion de vrais positifs détectés parmi tous les vrais positifs réels. Il est défini comme le rapport entre les vrais positifs (VP) et la somme des vrais positifs et des faux négatifs (FN):
Rappel = V P VP+FN
  26

Chapter 2. ANALYSE DE L’EXISTANT ET REVUE LITTERAIRE 2.3.4.3 Score F1 (F1 Score)
Le score F1 est une mesure harmonique de la précision et du rappel. Il offre un équilibre entre la précision et le rappel et est utile lorsque les classes sont déséquilibrées. Il est calculé comme la moyenne harmonique de la précision et du rappel:
F1 Score = 2PrécisionRappel Précision + Rappel
2.3.4.4 Courbe PR (PR Curve) et Aire sous la courbe PR (PR AUC)
La courbe PR est un graphique qui représente le rappel en fonction de la précision pour différentes valeurs de seuil de classification. Elle est utilisée pour évaluer la performance des modèles dans des situations de classes déséquilibrées. L’aire sous la courbe PR (PR AUC) mesure la performance globale du modèle pour différentes valeurs de seuil. Plus l’aire sous la courbe PR est élevée, meilleur est le modèle pour la classification de la classe minoritaire.
2.3.4.5 Moyenne de précision (Average Precision)
L’average precision est la moyenne des précisions calculées pour chaque seuil de classi- fication. C’est une mesure qui tient compte de l’ordre dans lequel les instances positives sont classées. Une average precision élevée indique une meilleure capacité du modèle à classer correctement les instances positives en haut de la liste.
Il est important de noter que dans le contexte de classes déséquilibrées, il faut accorder une attention particulière aux métriques qui évaluent la performance du modèle pour la classe minoritaire, car elles fournissent une indication plus précise de la capacité du modèle à détecter ces instances rares et importantes.
2.4 Conclusion :
Dans ce chapitre, nous avons traité les concepts et les méthodes qui seront indiqués ou utilisés pour réalisation du projet , nous avons également présenté les différents les algorithmes du Machine Learning que nous jugeons convenables pour les entrainer sur nos données collectés antérieurement
